---
title: "Mini-meta"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Goal: write code to conduct a mini meta-analysis (or meta regression) using just a few of our studies, as a proof of concept and to better understand how to work with the data and use the relevant R packages.

```{r libraries, echo=TRUE, output='hide', warning=FALSE, message=FALSE}
library(tidyverse)
library(metafor)
library(clubSandwich)
library(robumeta)
#library(kableExtra) #--> might be helpful later for making nice tables
```

## Import & clean data

```{r}
study_data = read.csv("mini-meta-practice-data.csv") %>% 
  janitor::clean_names(.) %>% 
  mutate_at(., vars(all_of(c("decade_of_majority_of_data"))), factor) %>% 
  mutate_at(., vars(all_of(c("exposure_operationalization",
                             "outcome_operationalization",
                             "source_population",
                             "which_model_is_this_from_in_their_analysis_table",
                             "confounders",
                             "notes_about_measure_of_association_type"))), list(as.character)) %>% 
  rename(.,
         sample_size = sample_size_observations,
         SE = standard_deviation_error,
         MA_type = measure_of_association_type,
         MA_number = measure_of_association_number)

knitr::kable(study_data)
```

## Look at effect sizes, transform using `escalc`

```{r}
knitr::kable(select(study_data,
                    study_id,
                    sample_size,
                    MA_type,
                    MA_number,
                    SE))

## Question: what are we trying to transform all of these *to*? Fisher's z? Cohen's d?
```

## Made-up dataset, for easier practicing
```{r}
set.seed(123)
replication_counts = c(3, 2, 1, 1, 1)

fake_data = tibble(
  es_id = 1001:1008,
  label = c(": Edu", ": Employment", ": Gov't rep", ": IPV", ": Homicide", "", "", ""),
  study_id = rep(1:5, replication_counts),
  authors = rep(c("Author1", "Author2", "Author3", "Author4", "Author1"), replication_counts),
  pub_date = as.character(rep(2001:2005, replication_counts)),
  unit_of_analysis = rep(c("City/MSA", "County", "Neighborhood", "County", "State"), replication_counts),
  setting = rep(c("Urban", "Rural", "Urban", "Rural", "USA"), replication_counts),
  outcome_data_source = rep(c("FBI", "Survey", "Survey", "Local PD", "FBI"), replication_counts),
  sample_size = rep(c(75, 100, 30, 150, 50), replication_counts),
  result = rnorm(8, mean = 1.03, sd = 0.3), #c(0.8, 0.85, 1.01, 2, 2.1, 0.95, 0.87, 1.5), #what are these??
  error = abs(rnorm(8, mean = 0.04, sd = 0.03)) #c(0.03, 0.04, 0.2, 0.2, 0.23, 0.04, 0.05, 0.01)
)

knitr::kable(fake_data)
```

## Using `metafor` and `clubsandwich`

References:

1. This working paper provides a very useful explanation and guide: https://www.jepusto.com/#working-papers 
2. Here's the clubSandwich documentation: https://cran.r-project.org/web/packages/clubSandwich/clubSandwich.pdf
3. And the metafor website: https://wviechtb.github.io/metafor/reference/rma.mv.html

**An inventory of decisions we're making:**

* What type of model?
  * Multivariate, becasuse some studies contributed multiple effect sizes (hence, `rma.mv`)
  * Random effects, because our studies represent a distribution of different true effects
* What is the correlation structure between estimates from the same study?
  * We don't know! (not enough info provided per study) --> so we estimate the fixed effects component as constant, rho = ?? (0.6/0.8)
  * Since we might be wrong with the assumption above, we use Robust Variance Estimation (RVE) when conducting our statistical tests
* What is the nested structure of our data?
  * One option would be "research group" --> studies --> effect sizes
  * For now, it's just studies --> effect sizes
  
*If we want to additionally cluster by "research group" (author? unclear whether this is appropriate in our case), the supplementary materials here have some guidance in S3.1: https://osf.io/nyv4u/*

```{r}
# constant sampling correlation assumption
rho <- 0.6 #is this a good assumption for us? I think in other places it's 0.8....

########################################################################################
# STEP 1: Identify an Appropriate Working Model
# --> CHE: multilevel random effects model with constant sampling correlation working model
########################################################################################


########################################################################################
# STEP 2: Estimate the Meta-Regression while Treating the Working Model as True
########################################################################################

# constant sampling correlation working model
cov_matrix <- impute_covariance_matrix(
  vi = fake_data$error, #vector of variances
  cluster = fake_data$study_id,
  r = rho, #assuming constant. make sure we pick the right value.
  smooth_vi = FALSE #this is the default. how would we know if we want to smooth?
  #subgroup = fake_data$authors, #i *think* this is where we'd do heirarchical clustering. but not sure, so leaving it out for now
)

# fit random effects working model in metafor
multilevel_model <- rma.mv(
  # here I'm specificying a formula, rather than an argument for the Y, and for the moderators
  result ~ 0 + outcome_data_source + setting, #"use a no-intercept specificaiton so that coefficients represent avg effect sizes for the corresponding category" (pre-print p.23)
  V = cov_matrix,
  random = ~ 1 | study_id / es_id, #this designates the nested structure of the random effects. QUESTION: if we do heirarchical + clustered, how do we designate that?
  #sparse = TRUE, #this is about speeding up the process, so not sure if it's necessary. I left it out.
  #test= "t", #use t-tests instead of the default z. why? (I left it out.)
  data = fake_data
)

multilevel_model # Note that this reports model-based (not robust) standard errors

########################################################################################
# STEP 3: Guard Against Misspecification by Using RVE
# --> use this to calculate standard errors, hypothesis tests, or CIs for the meta-reg. betas
########################################################################################

#these tests are RVE based and are robust to mispecification of the variances and covariances

coef_test(
  obj = multilevel_model, #estimation model above
  #cluster = fake_data$study_id, #define cluster IDs (not needed, already specified in model)
  vcov = "CR2" #estimation method (CR2 is best)
)

conf_int(
  obj = multilevel_model,
  vcov = "CR2"
)

#Not sure when we would do a wald test, or what the constraints would be....
# Wald_test(
#   obj = multilevel_model,
#   constraints = ??
#   vcov = "CR2"
# )
```

**Plots**

```{r funnel_plot}
# For the funnel and forest plots, it makes more sense (to me) to plot a version that doesn't include moderators.
no_moderators <- rma.mv(
  result ~ 1,
  V = cov_matrix,
  random = ~ 1 | study_id / es_id, #this designates the nested structure of the random effects. QUESTION: if we do heirarchical + clustered, how do we designate that?
  data = fake_data
)
#no_moderators

funnel(no_moderators)
ranktest(no_moderators)
# QUESTION1: should we also do a statistical test for publication bias? If so,is it this one?
# QUESTION2: is this still appropriately down-weighting estimates within the same study? I don't think so, because each dot just shows up... but maybe it doesn't matter?
```

Note: there are a lot of really neat options on the metafor website for forest plots! https://wviechtb.github.io/metafor/reference/forest.rma.html The very last example has the nested structure (which fits our data), but is also the least beautiful. There might be a way to use some of the different preferences and options to make ours nice.
```{r forest_plot}
forest(
  x = rma.mv(result ~ 1, V = cov_matrix, random = ~ 1 | study_id / es_id, data = fake_data,
             slab=paste0(authors, " (", pub_date, ")", label)),
  annotate = TRUE, #does this add numbers on to the plot?
  addfit = FALSE, #TRUE would add in one summary measure for the whole thing
  #addpred = FALSE, #the bounds of the prediction interval (might not be meaningful)
  showweights = FALSE, #TRUE would be interesting to see, but the documentation warns the weights shown don't reflect the complexity from the model with rma.mv
  #level=x$level, #this is about the CI level, which I don't think is included anywhere in our model? even though that's where the default is taken from
  refline = 1, #I put this at 1 to show the relation to the null. change if not using ORs
  #digits=2L, #rounding for annotations
  #xlab, #label for the x-axis
  #slab = fake_data$study_id, #study labels. supress with NA, or add column
  #ilab, #optional extra label for studies
  #ilab.xpos, ilab.pos,
  order = c(1:3, 8, 4:7), #specify how the studies should be ordered, including obs (effect sizes), prec (variances), or a vector with the order
  header = TRUE #can pass in a character vector for left & right headings
)

```

---------------------
## Using `robumeta`

Documentation: https://cran.r-project.org/web/packages/robumeta/robumeta.pdf

**Set up regression models**
```{r robu1, eval=FALSE}
# Meta-Regression model: Effect_Size = B0 + B1*unit_of_analysis + B2*setting + B3*outcome_data_source + u + e
# --> some are confounders, others are explanatory variables
# ACTUALLY: that fake regression model couldn't be solved (data points too similar)
# --> so I'm just doing a model with one predictor: Effect_Size = B0 + B1*outcome_data_source + u + e

fake_model = robu(
  formula = result ~ outcome_data_source,#unit_of_analysis + setting + outcome_data_source,
  data = fake_data,
  studynum = study_id, #studynum must be a numeric or factor variable that uniquely identifies each study
  var.eff.size = error, #A vector of user-calculated effect-size variances.
  modelweights = "CORR", #a choice of corr or hier. TODO: after all data is extracted, check to make sure the balance really falls towards this side in our data.
  rho = 0.8, #this is the default value. explain what it is and why the default is appropriate
  small = TRUE #this is the default. how do we know if we want the small sample correction?
  # QUESTION: how to account for heirarchical clustering in our sample (i.e. same author1/5)?
)

fake_model_INTERCEPT_ONLY = robu(
  formula = result ~ 1,
  data = fake_data,
  studynum = study_id, #studynum must be a numeric or factor variable that uniquely identifies each study
  var.eff.size = error, #A vector of user-calculated effect-size variances.
  modelweights = "CORR", #a choice of corr or hier. TODO: after all data is extracted, check to make sure the balance really falls towards this side in our data.
  rho = 0.8, #this is the default value. explain what it is and why the default is appropriate
  small = TRUE #this is the default. how do we know if we want the small sample correction?
  # QUESTION: how to account for heirarchical clustering in our sample (i.e. same author1/5)?
)

```

**Results & plots**

```{r robu2, eval=FALSE}
# NOTE: I think we might not want to use robumeta's forest plot, because it's not very customizeable (I'd like to remove the summary measure from the bottom, and maybe also even not have the summary dashed line?)
# HOWEVER: I'm not sure if we're allowed to do our meta-regression with robumeta and make our plots a different way.

# forest plot (w/summary measure)
forest_plot = forest.robu(
  fake_model_INTERCEPT_ONLY,
  es.lab = "es_id",
  study.lab = "study_id"
)

# funnel plot (publication bias)
# NOTE: robumeta doesn't have a funnel plot function.

# meta regression coefs/results
fake_model
```


