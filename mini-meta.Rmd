---
title: "Mini-meta"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Goal: write code to conduct a mini meta-regression using just a few of our studies, as a proof of concept and to better understand how to work with the data and use the relevant R packages.

<details>
<summary> Click to expand and see the initial data import and cleaning steps & full table from the data extraction form. </summary>

```{r libraries, echo=TRUE, output='hide', warning=FALSE, message=FALSE}
library(tidyverse)
library(metafor)
library(clubSandwich)
library(robumeta)
#library(kableExtra) #--> might be helpful later for making nice tables
```

```{r}
study_data = read.csv("mini-meta-practice-data.csv") %>% 
  janitor::clean_names(.) %>% 
  rowid_to_column(., "effect_size_id") %>% 
  mutate_at(., vars(all_of(c("decade_of_majority_of_data"))), factor) %>% 
  mutate_at(., vars(all_of(c("exposure_operationalization",
                             "outcome_operationalization",
                             "source_population",
                             "which_model_is_this_from_in_their_analysis_table",
                             "confounders",
                             "notes_about_measure_of_association_type"))), list(as.character)) %>% 
  rename(.,
         sample_size = sample_size_observations,
         SE = standard_deviation_error,
         MA_type = measure_of_association_type,
         MA_number = measure_of_association_number)

knitr::kable(study_data)
```
</details>

<details>
<summary> DELETE THIS ONCE I'VE REPLACED THE FAKE DATA WITH REAL DATA </summary>

## Made-up dataset, for easier practicing
```{r}
set.seed(123)
replication_counts = c(3, 2, 1, 1, 1)

fake_data = tibble(
  es_id = 1001:1008,
  study_id = rep(1:5, replication_counts),
  authors = rep(c("Author1", "Author2", "Author3", "Author4", "Author1"), replication_counts),
  pub_date = as.character(rep(2001:2005, replication_counts)),
  unit_of_analysis = rep(c("City/MSA", "County", "Neighborhood", "County", "State"), replication_counts),
  setting = rep(c("Urban", "Rural", "Urban", "Rural", "USA"), replication_counts),
  outcome_data_source = rep(c("FBI", "Survey", "Survey", "Local PD", "FBI"), replication_counts),
  sample_size = rep(c(75, 100, 30, 150, 50), replication_counts),
  #num_predictors = c(??????), #number of predictors in the regression model
  result = rnorm(8, mean = 1.03, sd = 0.3),
  error = abs(rnorm(8, mean = 0.04, sd = 0.03)),
  label = c(": Edu", ": Employment", ": Gov't rep", ": IPV", ": Homicide", "", "", "")
)

knitr::kable(fake_data)
```
</details>

## Step 1: Create a sample summary table of included studies
```{r}
# choose what information to include
# condense to one line per study
# note the number of effects that that study contributed
# order alphabetically
```


## Step 2: Transform effect sizes into a uniform statistic

First, look at the information we have on these effect sizes:
```{r}
knitr::kable(select(study_data,
                    study_id,
                    sample_size,
                    MA_type,
                    MA_number,
                    SE))
```

**We are going to convert all effect sizes into partial corerlation coefficients.**

<details>
<summary> Click for references, instructions, & code. </summary>

References:

- Aloe & Thompson (2013): https://www.journals.uchicago.edu/doi/10.5243/jsswr.2013.24
- Aloe & Becker (2012): https://journals.sagepub.com/doi/10.3102/1076998610396901
- Aloe (2013): https://www.tandfonline.com/doi/abs/10.1080/00221309.2013.853021 

From the `metafor` website's documentation on the `escalc` function:

> To compute these measures, one needs to specify `ti` for the test statistics (i.e., t-tests) of the regression coefficient of interest, `ni` for the sample sizes of the studies, `mi` for the number of predictors in the regression models... The options for the measure argument are then: "PCOR" for the partial correlation coefficient...

For the majority of our studies, we will have:

* Beta
* Standard error
* Number of predictors in the regression model
* Sample size

This means we'll need to convert effect sizes in two steps: (1) get the t-test statistic using Beta & standard error, (2) use `escalc` to compute a partial correlation coefficient.

*Note: for a handful of studies, they already provide partial correlation coefficients. For others, they provide 2x2 tables or risk differences. In these cases, we can use other `escalc` options to convert between d-family and r-family effects (not shown here yet).*
```{r}
# Step 1: get the t-test statistic
# Step 2: use `escalc` to compute partial correlation coefficients. add on to data table.
```
</details>

<br>
Here is our standardized data, ready for analysis:
```{r}
#knitr::kable(??)
```

*Note: We can see here that some effects are positive [supporting the amelioration hypothesis] and others are negative [supporting the backlash hypothesis]. Effects from study ?? have higher variance than the rest, so later on in the forest plot we should expect to see them have wider confidence intervals.*

## Build our regression model, conduct statistical tests, create plots

(Using the `metafor` and `clubsandwich` packages)

References:

1. This working paper provides a very useful explanation and guide: https://www.jepusto.com/#working-papers 
2. This is a reference for the statistical motivation, from *Research Synthesis Methods*: https://onlinelibrary.wiley.com/doi/abs/10.1002/jrsm.5
3. Documentation for `clubSandwich`: https://cran.r-project.org/web/packages/clubSandwich/clubSandwich.pdf
3. Documentation for `metafor`: https://wviechtb.github.io/metafor/reference/rma.mv.html

**An inventory of choices we're making to decide how to build the meta-regression model:**

* What type of model?
  * Multivariate/multi-level, becasuse some studies contributed multiple effect sizes (hence, `rma.mv`)
  * Random effects, because our studies represent a distribution of *different* true effects
* What is the correlation structure between estimates from the same study?
  * We don't know! (not enough info provided per study) --> so we estimate the fixed effects component as constant, rho = ?? (some references suggest `0.6`, some `0.8`, so we'll think about it)
  * Since we might be wrong with the assumption above, we use Robust Variance Estimation (RVE) when building our model and conducting our statistical tests
* What is the nested/heirarchcal structure of our data?
  * For now, it's: `studies --> effect sizes`
  * Another option would be `"research group"/author --> studies --> effect sizes` (this is suggested when the same research group is looking at multiple different cohorts for different studies. there aren't really "research groups", although some authors frequently publish together or have contributed multiple papers to our analysis) *Note: If we want to do this, the supplementary materials here have some guidance in S3.1: https://osf.io/nyv4u/*

```{r}
# constant sampling correlation assumption
rho <- 0.6 #is this a good assumption for us? In other places it's 0.8....

# create a covariance matrix assuming constant sampling correlation (working model)
cov_matrix <- impute_covariance_matrix(
  vi = fake_data$error, #vector of variances
  cluster = fake_data$study_id,
  r = rho, #assuming constant. make sure we pick the right value.
  smooth_vi = FALSE #this is the default. how would we know if we want to smooth?
  #subgroup = fake_data$authors, #i *think* this is where we'd do heirarchical clustering. but not sure, so leaving it out for now
)

# fit random effects working model in metafor
multilevel_model <- rma.mv(
  # here I'm specificying a formula, rather than an argument for the Y, and for the moderators
  result ~ 0 + outcome_data_source + setting, #"use a no-intercept specificaiton so that coefficients represent avg effect sizes for the corresponding category" (pre-print p.23)
  V = cov_matrix,
  random = ~ 1 | study_id / es_id, #this designates the nested structure of the random effects. QUESTION: if we do heirarchical + clustered, how do we designate that?
  #sparse = TRUE, #this is about speeding up the process, so not sure if it's necessary. I left it out.
  #test= "t", #use t-tests instead of the default z. why would we want this? (I left it out.)
  data = fake_data
)
```

### Meta-regression output:
```{r}
multilevel_model # Note that this reports model-based (not robust) standard errors
```

### T-tests for each variable:
```{r}
#these tests are RVE based and are robust to mispecification of the variances and covariances
coef_test(
  obj = multilevel_model, #estimation model above
  #cluster = fake_data$study_id, #define cluster IDs (not needed, already specified in model)
  vcov = "CR2" #estimation method (CR2 is best)
)
```

### Confidence intervals for those tests:
```{r}
conf_int(
  obj = multilevel_model,
  vcov = "CR2"
)
```
*Note: Here we might interpret one of these results as ???? [of course, this isn't meaningful right now with just 4 studies!]*

```{r, echo=FALSE}
#Not sure when we would do a wald test, or what the constraints would be....
# Wald_test(
#   obj = multilevel_model,
#   constraints = ??
#   vcov = "CR2"
# )
```

### Funnel plot: publication bias
```{r funnel_plot}
# For the funnel and forest plots, it makes more sense (to me) to plot from a regression model that doesn't include moderators.
no_moderators <- rma.mv(
  result ~ 1,
  V = cov_matrix,
  random = ~ 1 | study_id / es_id, #this designates the nested structure of the random effects. QUESTION: if we do heirarchical + clustered, how do we designate that?
  data = fake_data
)

funnel(no_moderators)
ranktest(no_moderators)
# QUESTION1: should we also do a statistical test for publication bias? If so, is it this one?
# QUESTION2: is this still appropriately down-weighting estimates within the same study? I don't think so, because each dot just shows up... but maybe it doesn't matter?
```

### Forest plot!

*Note: there are a lot of [really neat options](https://wviechtb.github.io/metafor/reference/forest.rma.html) on the metafor website for forest plots! The very last example has the nested structure (which fits our data), but is also the least beautiful. There might be a way to use some of the different preferences and options to make ours nice...*

```{r forest_plot}
forest(
  x = rma.mv(result ~ 1, V = cov_matrix, random = ~ 1 | study_id / es_id, data = fake_data,
             slab=paste0(authors, " (", pub_date, ")", label)),
  annotate = TRUE, #does this add numbers on to the plot?
  addfit = FALSE, #TRUE would add in one summary measure for the whole thing
  #addpred = FALSE, #the bounds of the prediction interval (might not be meaningful)
  showweights = FALSE, #TRUE would be interesting to see, but the documentation warns the weights shown don't reflect the complexity from the model with rma.mv
  #level=x$level, #this is about the CI level, which I don't think is included anywhere in our model? even though that's where the default is taken from
  refline = 1, #I put this at 1 to show the relation to the null. change if not using ORs
  #digits=2L, #rounding for annotations
  #xlab, #label for the x-axis
  #slab = fake_data$study_id, #study labels. supress with NA, or add column
  #ilab, #optional extra label for studies
  #ilab.xpos, ilab.pos,
  order = c(1:3, 8, 4:7), #specify how the studies should be ordered, including obs (effect sizes), prec (variances), or a vector with the order
  header = c("Study (& specific effect)", "Estimate [95% CI]") #can pass in a character vector for left & right headings, or TRUE
)
```
